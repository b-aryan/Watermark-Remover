# Watermarkâ€‘RemoverÂ (GANâ€based, Patchâ€‘wise)

> Patchâ€‘based adversarial model that learns to **erase watermarks** from images of any size.  
> Combines a Resâ€‘UNet generator, a spectralâ€‘norm PatchGAN discriminator, and VGGâ€‘19 perceptual loss.

---

## âœ¨ Sample Results

Here's a glimpse of the model's watermark removal capabilities:

| Before (Watermarked) | After (Restored by Model) |
| :------------------: | :-----------------------: |
| ![Watermarked Example 1](./samples/1_w.jpg) | ![Restored Example 1](./samples/1_r.jpg) |
| ![Watermarked Example 3](./samples/4_w.jpg) | ![Restored Example 3](./samples/4_r.jpg) |
| ![Watermarked Example 5](./samples/6_w.jpg) | ![Restored Example 5](./samples/6_r.jpg) |
| ![Watermarked Example 6](./samples/7_w.jpg) | ![Restored Example 6](./samples/7_r.jpg) |

#### ğŸ”— Live Demo

Try the model directly in your browser at:  
ğŸ‘‰ [**WM-Remover on Hugging Face Spaces**](https://huggingface.co/spaces/b-aryan/WM-Remover)

You can upload or paste a link to a watermarked image, and the model will return a (mostly) clean, restored version.

---

## âœ¨ Key Features
| Feature | Why it matters |
|---------|---------------|
| **Resâ€‘UNetÂ Generator** â€“ 4â€‘level encoder/decoder with 9 residual blocks (stochasticâ€‘depth)| Preserves spatial detail while boosting capacity. |
| **Singleâ€‘scale PatchGAN Discriminator** | Penalises local artefacts without overâ€‘parameterisation. |
| **Perceptual + L1 + GAN losses** | Balances pixel fidelity with perceptual realism. |
| **Patchâ€‘wise training** (`256Â Ã—Â 256` kernels, `64`Â px stride) | Handles arbitrarily large images; smaller GPU footprint. |
| **Albumentations pipeline** | Realâ€world JPEG, noise, motion blur, flips, etc. |
| **AMP + gradient clipping + schedulers** | Stable, mixedâ€‘precision training out of the box. |

---

## ğŸ“‚ Directory Layout

```text
.
â”œâ”€â”€ dataset-smol/              # default sample dataset (see below)
â”‚   â”œâ”€â”€ mark/                  # watermarked images  â†’  {id}_c.jpg
â”‚   â””â”€â”€ nomark/                # pristine targets    â†’  {id}_r.jpg
â”œâ”€â”€ cropper.py                 # optional preâ€‘processor to align pairs
â”œâ”€â”€ dataset.py                 # WatermarkPatchDataset  (patch extraction)
â”œâ”€â”€ generator_model.py         # Resâ€‘UNet G
â”œâ”€â”€ discriminator_model.py     # PatchGAN D
â”œâ”€â”€ vgg_loss.py                # perceptual loss (VGGâ€‘19)
â”œâ”€â”€ config.py                  # all tunables live here
â”œâ”€â”€ train.py                   # main training script
â”œâ”€â”€ utils.py                   # checkpoint helpers
â”œâ”€â”€ run_on_patches_online.py   # Doing inference
â””â”€â”€ README.md
```
## ğŸ—ï¸Â Model Architecture

| Component | Design | RationaleÂ /Â Diff.Â vsÂ Pix2PixHD |
|-----------|--------|--------------------------------|
| **GeneratorÂ (Resâ€‘UNet)** | â€¢ 7Ã—7 reflectionâ€‘padded stem<br>â€¢ **Encoder:** four downâ€‘sampling blocks (`64Â â†’Â 1024` channels) â€“ each 3Ã—3Â convÂ +Â BNÂ +Â ReLU, strideÂ 2<br>â€¢ **Bottleneck:** 9 ResidualBlocks with **stochastic depthÂ pÂ =Â 0.8**<br>â€¢ **Decoder:** nearestâ€‘neighbour upâ€‘sample â†’ 3Ã—3Â conv (strideÂ 1) with UNet skipâ€‘concats<br>â€¢ Final 7Ã—7Â convÂ +Â tanh | â€¢ Keeps Pix2PixHDâ€™s residual core but drops multiâ€‘scale encoder; uses a shallower 4â€‘level UNet to preserve fine structure.<br>â€¢ Stochastic depth regularises the deep residual stack (absent in Pix2Pix/HD).<br>â€¢ Uses NNâ€‘upsampleÂ +Â conv to avoid checkerâ€‘board artefacts from transposed conv. |
| **DiscriminatorÂ (Spectralâ€‘Norm PatchGAN)** | â€¢ Concatenates input & target (6Â ch) â†’ 3Ã—3Â conv stack: 64Â â†’Â 1024Â ch, strideÂ 2 every other block<br>â€¢ Spectralâ€‘norm on **every** conv layer | â€¢ Singleâ€‘scale only (Pix2PixHD uses 3); halves VRAM and speeds training.<br>â€¢ SpectralNorm replaces WeightNorm for stronger Lipschitz control. |
| **Losses** | **â„’<sub>GAN</sub>**: BCE on Dâ€™s patch logits<br>**â„’<sub>1</sub>**: pixel fidelity<br>**â„’<sub>perc</sub>**: multiâ€‘layer VGGâ€‘19 (slicesÂ 1â€‘5) with layerâ€‘wise weights 1/32â€¦1 | Matches Pix2PixHD recipe, but weights are exposed in `config.py` for easy tuning. |
| **Patch Training Strategy** | Extracts 256Ã—256 patches with 64Â px stride (see `WatermarkPatchDataset`) so the model can handle arbitrarily large images on 8Â GB GPUs. | Pix2PixHD trains on full images; patchâ€‘wise training yields more updates per epoch and sharper watermark localisation. |

### Why this matters
* **Sharper restoration** â€“ residual depth + perceptual loss remove ghosts without overâ€‘smoothing.  
* **Stable training** â€“ spectralâ€‘norm, stochastic depth, AMP, and a cosine LR scheduler keep GAN loss curves smooth.

> **TIP:** To tweak capacity, change `features` (base channel count) and `num_residuals` inÂ `generator_model.py`; everything else scales automatically.


## ğŸ‹ï¸ Training Details

Training was on [Kaggle Platform](https://kaggle.com) using their `T4x2 GPUs`.

- **Input Size**: 256 Ã— 256 patches  
  Extracted from larger images using a custom `WatermarkPatchDataset`. Clean targets are resized using `LANCZOS` to match watermarked patch dimensions.

- **Optimizer**:
  - Generator: `Adam` (lr = `2e-4`, betas = `(0.5, 0.999)`)
  - Discriminator: `Adam` (same settings)

- **Schedulers**:
  - **ReduceLROnPlateau** for both Generator and Discriminator
  - Patience: 2 epochs without improvement triggers LR reduction

- **Loss Functions**:
  - **L1 Loss** (pixel-wise)
  - **GAN Loss**: Binary Cross Entropy with logits
  - **Perceptual Loss**:
    - Uses pretrained **VGG19** from PyTorch
    - Slices features at layers `[1, 6, 11, 20, 29]`
    - Layer weights: `[1/32, 1/16, 1/8, 1/4, 1]`

- **Mixed Precision Training**:
  - Enabled via `torch.cuda.amp` for both Generator and Discriminator
  - Helps reduce memory footprint and accelerate training

- **Batch Size**: Configurable (default = 8)

- **Epochs**: Configurable

- **Checkpointing**:
  - Generator and Discriminator weights saved every few epochs
  - `utils.py` provides `save_checkpoint()` and `load_checkpoint()` for resuming training

> Training is done patch-wise to generalize across various image resolutions and enhance local feature restoration.

## ğŸš€ Inference

This model is designed to handle arbitrarily large images by processing them in **overlapping patches** and seamlessly reconstructing the output using `run_on_patches_online.py`.

